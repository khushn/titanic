{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions\n",
    "Some util functions like Sigmoid, Relu, and their backwards used in Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(1) (0.7310585786300049, 1)\n",
      "sigmoid(-1) (0.2689414213699951, -1)\n",
      "sigmoid(0) (0.5, 0)\n",
      "sigmoid(100) (1.0, 100)\n",
      "sigmoid(-100) (3.7200759760208356e-44, -100)\n",
      "sigmoid(arr) (array([ 0.7310586 ,  0.5       ,  0.26894143], dtype=float32), array([ 1.,  0., -1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    res = 1./ (1 + np.exp(-x))\n",
    "    activation_cache = x\n",
    "    return res, activation_cache\n",
    "\n",
    "print('sigmoid(1)', sigmoid(1))\n",
    "print('sigmoid(-1)', sigmoid(-1))\n",
    "print('sigmoid(0)', sigmoid(0))\n",
    "print('sigmoid(100)', sigmoid(100))\n",
    "print('sigmoid(-100)', sigmoid(-100))\n",
    "y=np.array([1, 0, -1], dtype=np.float32)\n",
    "print('sigmoid(arr)', sigmoid(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(1) (1.0, 1)\n",
      "relu(-1) (0.0, -1)\n",
      "relu(0) (0.0, 0)\n",
      "relu(100) (100.0, 100)\n",
      "relu(-100) (0.0, -100)\n",
      "relu(arr) (array([ 2.,  0.,  0.], dtype=float32), array([ 2.,  0., -1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    res= np.maximum(0., x)\n",
    "    activation_cache = x\n",
    "    return res, activation_cache\n",
    "        \n",
    "print('relu(1)', relu(1))\n",
    "print('relu(-1)', relu(-1))\n",
    "print('relu(0)', relu(0))\n",
    "print('relu(100)', relu(100))\n",
    "print('relu(-100)', relu(-100))\n",
    "y=np.array([2, 0, -1], dtype=np.float32)\n",
    "print('relu(arr)', relu(y))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$. \n",
    "\n",
    "This link shows that derivative of sigmoid(x) is sig(x)*(1-sig(x):\n",
    "\n",
    "https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_backward: [ 0.          0.00132961]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z=activation_cache\n",
    "    sig, _ = sigmoid(Z)\n",
    "    sig_d = sig*(1-sig)\n",
    "    return dA * sig_d \n",
    "    \n",
    "dA=np.array([.7, .2], dtype=np.float32)\n",
    "a_cache=np.array([100., 5.], dtype = np.float32)\n",
    "print('sigmoid_backward:', sigmoid_backward(dA, a_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_backward: [ 0.69999999  0.        ]\n"
     ]
    }
   ],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    Z=activation_cache\n",
    "    Z[Z>0]=1.\n",
    "    Z[Z<=0]=0.\n",
    "    return dA * Z \n",
    "    \n",
    "dA=np.array([.7, .2], dtype=np.float32)\n",
    "a_cache=np.array([100., -5.], dtype = np.float32)\n",
    "print('relu_backward:', relu_backward(dA, a_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A)+ b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_forward_test_case' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-893d6ffd1271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward_test_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_forward_test_case' is not defined"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        W=parameters[\"W\" + str(l)]\n",
    "        b=parameters[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W=parameters[\"W\" + str(L)]\n",
    "    b=parameters[\"b\" + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -np.sum( Y*np.log(AL) + (1-Y)*np.log(1-AL) )/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, A_prev.transpose())/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.transpose(), dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The application of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The app\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "print('The app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data[data=='female']=1\n",
    "    data[data=='male']=2\n",
    "    data[data=='C']=1\n",
    "    data[data=='Q']=2\n",
    "    data[data=='S']=3\n",
    "    data=np.array(data)\n",
    "    data[data=='']=0.0 # needed for any null values\n",
    "    data=data.astype(float)\n",
    "\n",
    "    print('data[1]', data[1])\n",
    "   \n",
    "    #Normalize the data\n",
    "    for i in range(data.shape[1]):\n",
    "        data[:, i] = data[:, i]/data[:, i].max()\n",
    "    \n",
    "    print('normalized data[1]', data[1])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  891\n",
      "2nd name:  Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n",
      "data[1] [  1.       1.       1.      38.       1.       0.      71.2833]\n",
      "normalized data[1] [ 1.          0.33333333  0.5         0.475       0.125       0.\n",
      "  0.13913574]\n",
      "data.shape:  (891, 7)\n",
      "num train:  712\n",
      "num validation:  179\n",
      "train_x.shape:  (6, 712)\n",
      "train_y.shape:  (1, 712)\n",
      "valid_x.shape:  (6, 179)\n",
      "valid_y.shape:  (1, 179)\n"
     ]
    }
   ],
   "source": [
    "import csv as csv\n",
    "\n",
    "csv_file_object = csv.reader(open('train.csv', 'rt')) \t# Load in the csv file\n",
    "header = next(csv_file_object) \t\t\t\t\t\t# Skip the fist line as it is a header\n",
    "data=[] \t\t\t\t\t\t\t\t\t\t\t\t# Create a variable to hold the data\n",
    "\n",
    "for row in csv_file_object: \t\t\t\t\t\t\t# Skip through each row in the csv file,\n",
    "    data.append(row[0:]) \t\t\t\t\t\t\t\t# adding each row to the data variable\n",
    "data = np.array(data) \t\t\t\t\t\t\t\t\t# Then convert from a list to an array.\n",
    "\n",
    "print('len: ', len(data[0:, 0]))\n",
    "print('2nd name: ', data[1, 3])\n",
    "\n",
    "#data = data[:,[1, 2, 4, 5, 6, 7, 9, 11] ] # All relevant columns\n",
    "#data = data[:,[1, 2, 4, 5, 6, 7, 11] ] # If Not using fare\n",
    "data = data[:,[1, 2, 4, 5, 6, 7, 9] ] #Not using embarked\n",
    "\n",
    "data=preprocess(data)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "print('data.shape: ', data.shape)\n",
    "\n",
    "#80% as tarining data\n",
    "num_train=int(.8*len(data))\n",
    "\n",
    "\n",
    "train_data=data[0:num_train]\n",
    "validation_data=data[num_train:]\n",
    "\n",
    "print('num train: ', len(train_data))\n",
    "print('num validation: ', len(validation_data))\n",
    "\n",
    "train_x=train_data[:, 1:]\n",
    "train_y=np.array([train_data[:, 0]])\n",
    "train_x = train_x.transpose() # the model expects inputs this way \n",
    "print('train_x.shape: ', train_x.shape)\n",
    "print('train_y.shape: ', train_y.shape)\n",
    "\n",
    "valid_x=validation_data[:, 1:]\n",
    "valid_y=np.array([validation_data[:, 0]])\n",
    "valid_x = valid_x.transpose() # the model expects inputs this way \n",
    "\n",
    "print('valid_x.shape: ', valid_x.shape)\n",
    "print('valid_y.shape: ', valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [train_x.shape[0], 50, 1] #  n-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 2000 training example\n",
    "        if print_cost and i % 2000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 2000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692914\n",
      "Cost after iteration 2000: 0.665078\n",
      "Cost after iteration 4000: 0.656923\n",
      "Cost after iteration 6000: 0.637191\n",
      "Cost after iteration 8000: 0.601919\n",
      "Cost after iteration 10000: 0.556467\n",
      "Cost after iteration 12000: 0.520066\n",
      "Cost after iteration 14000: 0.496934\n",
      "Cost after iteration 16000: 0.482925\n",
      "Cost after iteration 18000: 0.474143\n",
      "Cost after iteration 20000: 0.468414\n",
      "Cost after iteration 22000: 0.464596\n",
      "Cost after iteration 24000: 0.462039\n",
      "Cost after iteration 26000: 0.460316\n",
      "Cost after iteration 28000: 0.459147\n",
      "Cost after iteration 30000: 0.458338\n",
      "Cost after iteration 32000: 0.457780\n",
      "Cost after iteration 34000: 0.457390\n",
      "Cost after iteration 36000: 0.457127\n",
      "Cost after iteration 38000: 0.456945\n",
      "Cost after iteration 40000: 0.456819\n",
      "Cost after iteration 42000: 0.456728\n",
      "Cost after iteration 44000: 0.456661\n",
      "Cost after iteration 46000: 0.456609\n",
      "Cost after iteration 48000: 0.456569\n",
      "Cost after iteration 50000: 0.456535\n",
      "Cost after iteration 52000: 0.456506\n",
      "Cost after iteration 54000: 0.456481\n",
      "Cost after iteration 56000: 0.456460\n",
      "Cost after iteration 58000: 0.456441\n",
      "Cost after iteration 60000: 0.456424\n",
      "Cost after iteration 62000: 0.456410\n",
      "Cost after iteration 64000: 0.456396\n",
      "Cost after iteration 66000: 0.456384\n",
      "Cost after iteration 68000: 0.456372\n",
      "Cost after iteration 70000: 0.456363\n",
      "Cost after iteration 72000: 0.456355\n",
      "Cost after iteration 74000: 0.456349\n",
      "Cost after iteration 76000: 0.456343\n",
      "Cost after iteration 78000: 0.456338\n",
      "Cost after iteration 80000: 0.456332\n",
      "Cost after iteration 82000: 0.456328\n",
      "Cost after iteration 84000: 0.456323\n",
      "Cost after iteration 86000: 0.456318\n",
      "Cost after iteration 88000: 0.456313\n",
      "Cost after iteration 90000: 0.456308\n",
      "Cost after iteration 92000: 0.456304\n",
      "Cost after iteration 94000: 0.456299\n",
      "Cost after iteration 96000: 0.456294\n",
      "Cost after iteration 98000: 0.456289\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAEWCAYAAAA5Am/SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbf56305c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate = .005, num_iterations = 100000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x, y, params):\n",
    "    al, _ = L_model_forward(x, parameters)    \n",
    "    al[al<=.5]=0\n",
    "    al[al>.5]=1\n",
    "    if y != None:\n",
    "        assert(al.shape == y.shape)\n",
    "        numy=y.shape[1]\n",
    "        print('numy: ', numy)\n",
    "        total = np.sum(np.abs(y-al))\n",
    "        print('total diff: ', total)\n",
    "        print('Accuracy: ', 1. - total/ numy)\n",
    "        pos=2\n",
    "        print('pos: ', pos, al[0, pos], y[0, pos])\n",
    "    return al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numy:  712\n",
      "total diff:  152.0\n",
      "Accuracy:  0.786516853933\n",
      "pos:  2 1.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khush/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numy:  179\n",
      "total diff:  36.0\n",
      "Accuracy:  0.798882681564\n",
      "pos:  2 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khush/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "valid_train = predict(valid_x, valid_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Trying it on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  418\n",
      "data[1] [  3.   1.  47.   1.   0.   7.]\n",
      "normalized data[1] [ 1.          0.5         0.61842105  0.125       0.          0.01366309]\n"
     ]
    }
   ],
   "source": [
    "import csv as csv\n",
    "\n",
    "csv_file_object = csv.reader(open('test.csv', 'rt')) \t# Load in the csv file\n",
    "header = next(csv_file_object) \t\t\t\t\t\t# Skip the fist line as it is a header\n",
    "data=[] \t\t\t\t\t\t\t\t\t\t\t\t# Create a variable to hold the data\n",
    "\n",
    "for row in csv_file_object: \t\t\t\t\t\t\t# Skip through each row in the csv file,\n",
    "    data.append(row[0:]) \t\t\t\t\t\t\t\t# adding each row to the data variable\n",
    "data = np.array(data) \t\t\t\t\t\t\t\t\t# Then convert from a list to an array.\n",
    "\n",
    "\n",
    "\n",
    "ids=data[:,0]\n",
    "print('len: ', len(ids))\n",
    "\n",
    "#Note in test data columns are diff than in train\n",
    "data = data[:,[1, 3, 4, 5, 6, 8] ] #Not using embarked\n",
    "\n",
    "data=preprocess(data)\n",
    "test_x=data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results\n",
      "Output written to file:  test_results.csv\n"
     ]
    }
   ],
   "source": [
    "test_res = predict(test_x, None, parameters)\n",
    "print('Test Results')\n",
    "ids=ids.astype(int)\n",
    "ids=np.array(ids, dtype=int)\n",
    "test_res=np.array(test_res, dtype=int)\n",
    "#print(ids)\n",
    "#print(test_res)\n",
    "out=np.zeros((len(ids), 2), dtype=int)\n",
    "out[:,0]=ids\n",
    "out[:,1]=test_res\n",
    "#print(out)\n",
    "\n",
    "#Write the output to results file\n",
    "test_out_file=\"test_results.csv\"\n",
    "with open(test_out_file, \"wb\") as f:\n",
    "    f.write(b'PassengerId,Survived\\n')\n",
    "    np.savetxt(f, out.astype(int), fmt='%i', delimiter=\",\")\n",
    "\n",
    "print(\"Output written to file: \", test_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
